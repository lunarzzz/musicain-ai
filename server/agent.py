"""Agent ç¼–æ’æ ¸å¿ƒ â€” æ„å›¾è¯†åˆ« â†’ å·¥å…·è°ƒç”¨ â†’ ç»“æœåˆæˆ â†’ å“åº”ç»„è£…

é‡‡ç”¨ LangChain çš„ create_tool_calling_agentï¼Œé…åˆ Function Calling
å®ç°å¤šæ­¥æ¨ç†ä¸å·¥å…·è°ƒåº¦ã€‚
"""

from __future__ import annotations

import json
import uuid
from typing import AsyncGenerator

from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI

from config import settings
from models import CardData, CardType, Evidence, StreamChunk
import database as db
from skill_loader import load_all_skills

# â”€â”€ å¯¼å…¥æ‰€æœ‰ Tools â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from tools.hot_trends import get_trending_topics, generate_song_inspiration, generate_promo_tags
from tools.promotion import recommend_songs_to_promote, generate_promotion_plan, get_promotion_report
from tools.analytics import get_audience_portrait, analyze_cross_platform, explain_metric_change
from tools.knowledge import search_knowledge, check_upload_compliance, ragflow_search

ALL_TOOLS = [
    get_trending_topics,
    generate_song_inspiration,
    generate_promo_tags,
    recommend_songs_to_promote,
    generate_promotion_plan,
    get_promotion_report,
    get_audience_portrait,
    analyze_cross_platform,
    explain_metric_change,
    search_knowledge,
    ragflow_search,
    check_upload_compliance,
]

TOOL_MAP = {t.name: t for t in ALL_TOOLS}

# â”€â”€ System Prompt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SYSTEM_PROMPT = """ä½ æ˜¯ã€Œè…¾è®¯éŸ³ä¹äºº AI åŠ©æ‰‹ã€ï¼Œä¸€ä¸ªä¸“ä¸šçš„éŸ³ä¹äººå·¥ä½œæµ Copilotã€‚

## ä½ çš„èƒ½åŠ›
ä½ å¯ä»¥é€šè¿‡è°ƒç”¨å·¥å…·æ¥å¸®åŠ©éŸ³ä¹äººå®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š
1. **çƒ­ç‚¹åˆ›ä½œ**ï¼šè·å–çƒ­ç‚¹è¶‹åŠ¿ã€ç”Ÿæˆæ­Œåçµæ„Ÿã€ç”Ÿæˆå®£æ¨æ ‡ç­¾
2. **å®£æ¨å»ºè®®**ï¼šæ¨èæœ€å€¼å¾—å®£æ¨çš„æ­Œæ›²ã€ç”ŸæˆæŠ•æ”¾è®¡åˆ’ã€æŠ•åå¤ç›˜
3. **æ™ºèƒ½åˆ†æ**ï¼šå¬ä¼—ç”»åƒã€è·¨å¹³å°è¡¨ç°åˆ†æã€å…³é”®æŒ‡æ ‡å˜åŒ–å½’å› 
4. **é—®ç­”æŒ‡å—**ï¼šå›ç­”å…¥é©»ã€ä¸Šä¼ ã€å®¡æ ¸ã€ç»“ç®—ã€ç‰ˆæƒã€æ´»åŠ¨ç­‰é—®é¢˜

## è¡Œä¸ºå‡†åˆ™
- **å§‹ç»ˆæä¾›å¯æ‰§è¡Œçš„å»ºè®®**ï¼Œä¸è¦åªç»™ç¬¼ç»Ÿçš„æ–¹å‘
- **å¼•ç”¨æ•°æ®æ—¶æ ‡æ³¨æ¥æºå’Œå£å¾„**ï¼Œç¡®ä¿å¯ä¿¡åº¦
- **å¯¹äºè§„åˆ™ã€æµç¨‹ã€FAQç­‰çŸ¥è¯†ç±»é—®é¢˜ï¼Œä¼˜å…ˆä½¿ç”¨ ragflow_search å·¥å…·æ£€ç´¢æœ€æƒå¨æ–‡æ¡£**
- **è¯­æ°”ä¸“ä¸šä½†äº²åˆ‡**ï¼Œåƒä¸€ä½èµ„æ·±çš„éŸ³ä¹è¡Œä¸šå‰è¾ˆ
- **ä¸»åŠ¨æ¨èä¸‹ä¸€æ­¥åŠ¨ä½œ**ï¼Œå¸®éŸ³ä¹äººåšåˆ°"é—­ç¯"
- **å½“ä¸ç¡®å®šæ—¶ï¼Œè¯šå®è¯´æ˜**å¹¶å¼•å¯¼ç”¨æˆ·è”ç³»äººå·¥å®¢æœ
- å›å¤ä½¿ç”¨ä¸­æ–‡ï¼Œæ ¼å¼æ¸…æ™°ç¾è§‚ï¼Œå–„ç”¨ Markdown æ’ç‰ˆ

## é‡è¦è§„åˆ™
- ä¸è¦ç¼–é€ ä¸å­˜åœ¨çš„æ•°æ®ï¼Œåªä½¿ç”¨å·¥å…·è¿”å›çš„çœŸå®æ•°æ®
- å¦‚æœç”¨æˆ·çš„é—®é¢˜è¶…å‡ºä½ çš„èƒ½åŠ›èŒƒå›´ï¼Œå‹å¥½åœ°è¯´æ˜å¹¶å»ºè®®æ›¿ä»£æ–¹æ¡ˆ
- åœ¨ç»™å‡ºå»ºè®®æ—¶ï¼Œå°½é‡é™„å¸¦ç†ç”±å’Œä¾æ®

## å¯ç”¨çš„é«˜çº§æŠ€èƒ½ (Agent Skills)
é™¤äº†åŸºæœ¬å·¥å…·ï¼Œä½ è¿˜æœ‰ä¸€äº›é¢„å®šä¹‰çš„â€œæŠ€èƒ½ (Skills)â€ã€‚è¿™äº›æŠ€èƒ½ç”±å¤šæ­¥æµç¨‹ç»„æˆã€‚
å½“ç”¨æˆ·æ„å›¾åŒ¹é…ä»¥ä¸‹æŸä¸ªæŠ€èƒ½æ—¶ï¼Œä½ å¿…é¡»**ä¸¥æ ¼éµå¾ª**è¯¥æŠ€èƒ½çš„ `[æ‰§è¡Œæ­¥éª¤]` è¿›è¡Œé€æ­¥çš„å·¥å…·è°ƒç”¨ã€‚
æ¯ä¸ªæ­¥éª¤æ‰§è¡Œå®Œå¿…é¡»å‘ç”¨æˆ·è§£é‡Šå½“å‰ç»“æœï¼Œç„¶åå†å»è°ƒä¸‹ä¸€ä¸ªæ­¥éª¤çš„å·¥å…·ï¼ˆä¸è¦ä¸€æ¬¡æ€§è°ƒç”¨æ‰€æœ‰å·¥å…·ï¼Œè¦ç­‰å¾…å‰ä¸€ä¸ªå·¥å…·çš„ç»“æœï¼‰ã€‚

<agent_skills>
{skills_prompt}
</agent_skills>
"""


def _get_system_prompt() -> str:
    """åŠ¨æ€ç”ŸæˆåŒ…å«å½“å‰å·²åŠ è½½ Skills çš„ System Prompt"""
    skills = load_all_skills()
    skills_text = ""
    for s in skills:
        skills_text += f"### æŠ€èƒ½åç§°ï¼š{s.name}\n"
        skills_text += f"**æè¿°**: {s.description}\n"
        skills_text += f"**è§¦å‘è¯**: {', '.join(s.trigger_keywords)}\n"
        skills_text += f"**æ‰§è¡Œæ­¥éª¤ä¸æŒ‡ä»¤**:\n{s.instructions}\n\n"
    
    if not skills_text.strip():
        skills_text = "ç›®å‰æ²¡æœ‰æ³¨å†Œçš„é«˜çº§æŠ€èƒ½ã€‚"
        
    return SYSTEM_PROMPT.replace("{skills_prompt}", skills_text.strip())


# â”€â”€ Agent æ ¸å¿ƒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _get_llm() -> ChatOpenAI:
    return ChatOpenAI(
        api_key=settings.LLM_API_KEY,
        base_url=settings.LLM_BASE_URL,
        model=settings.LLM_MODEL,
        temperature=settings.LLM_TEMPERATURE,
        streaming=True,
    )


def _build_messages(history: list[dict], user_msg: str) -> list:
    """æ„å»º LangChain æ¶ˆæ¯åˆ—è¡¨"""
    messages = [SystemMessage(content=_get_system_prompt())]

    for msg in history[-20:]:  # ä¿ç•™æœ€è¿‘ 20 æ¡å†å²
        role = msg.get("role", "user")
        content = msg.get("content", "")
        if role == "user":
            messages.append(HumanMessage(content=content))
        elif role == "assistant":
            messages.append(AIMessage(content=content))

    messages.append(HumanMessage(content=user_msg))
    return messages


def _generate_title(user_msg: str) -> str:
    """ä»é¦–æ¡æ¶ˆæ¯ç”Ÿæˆä¼šè¯æ ‡é¢˜"""
    title = user_msg.strip()[:30]
    if len(user_msg) > 30:
        title += "..."
    return title


def _extract_cards(tool_name: str, tool_result: dict) -> list[dict]:
    """ä»å·¥å…·è°ƒç”¨ç»“æœæå–å¯å±•ç¤ºçš„å¡ç‰‡æ•°æ®"""
    cards = []

    if tool_name == "get_trending_topics":
        topics = tool_result.get("topics", [])
        cards.append({
            "card_type": "hot_trend",
            "title": "ğŸ”¥ çƒ­ç‚¹è¶‹åŠ¿",
            "data": {"topics": topics, "updated_at": tool_result.get("updated_at")},
            "actions": [{"label": "åŸºäºçƒ­ç‚¹ç”Ÿæˆçµæ„Ÿ", "action_type": "callback", "payload": {"action": "generate_inspiration"}}],
        })

    elif tool_name == "generate_song_inspiration":
        cards.append({
            "card_type": "hot_trend",
            "title": "ğŸ’¡ åˆ›ä½œçµæ„Ÿ",
            "data": tool_result,
            "actions": [{"label": "ç”Ÿæˆå®£æ¨æ ‡ç­¾", "action_type": "callback", "payload": {"action": "generate_tags"}}],
        })

    elif tool_name in ("recommend_songs_to_promote",):
        recs = tool_result.get("recommendations", [])
        cards.append({
            "card_type": "song_recommend",
            "title": "ğŸµ æ¨æ­Œå»ºè®®",
            "data": {"recommendations": recs, "diagnosis": tool_result.get("diagnosis")},
            "actions": [{"label": "ç”ŸæˆæŠ•æ”¾è®¡åˆ’", "action_type": "callback", "payload": {"action": "create_plan"}}],
        })

    elif tool_name == "generate_promotion_plan":
        cards.append({
            "card_type": "promotion_plan",
            "title": "ğŸ“‹ æŠ•æ”¾è®¡åˆ’",
            "data": tool_result,
            "actions": [{"label": "å¼€å§‹æŠ•æ”¾", "action_type": "deeplink", "url": "/promotion/create"}],
        })

    elif tool_name == "get_promotion_report":
        cards.append({
            "card_type": "data_report",
            "title": "ğŸ“Š å®£æ¨å¤ç›˜æŠ¥å‘Š",
            "data": tool_result,
            "actions": [{"label": "è¿½åŠ æŠ•æ”¾", "action_type": "deeplink", "url": "/promotion/create"}],
        })

    elif tool_name == "get_audience_portrait":
        cards.append({
            "card_type": "audience_portrait",
            "title": "ğŸ‘¥ å¬ä¼—ç”»åƒ",
            "data": tool_result,
            "actions": [],
        })

    elif tool_name == "analyze_cross_platform":
        cards.append({
            "card_type": "data_report",
            "title": "ğŸ“ˆ è·¨å¹³å°åˆ†æ",
            "data": tool_result,
            "actions": [],
        })

    elif tool_name == "explain_metric_change":
        cards.append({
            "card_type": "data_report",
            "title": "ğŸ“‰ æŒ‡æ ‡å˜åŒ–åˆ†æ",
            "data": tool_result,
            "actions": [],
        })

    elif tool_name in ("search_knowledge", "check_upload_compliance", "ragflow_search"):
        cards.append({
            "card_type": "knowledge",
            "title": "ğŸ“– çŸ¥è¯†è§£ç­”",
            "data": tool_result,
            "actions": [{"label": "è”ç³»å®¢æœ", "action_type": "deeplink", "url": "/support"}],
        })

    return cards


def _parse_follow_ups(raw: str) -> list[str]:
    """å°† LLM ç”Ÿæˆçš„åç»­å»ºè®®ï¼ˆé—®é¢˜æˆ–è¯é¢˜ï¼‰è§£æä¸ºåˆ—è¡¨ã€‚"""
    if not raw:
        return []

    text = raw.strip()
    if not text:
        return []

    # ä¼˜å…ˆ JSON æ ¼å¼
    try:
        data = json.loads(text)
        if isinstance(data, dict):
            items = data.get("suggestions") or data.get("questions") or data.get("topics")
        else:
            items = data
        if isinstance(items, list):
            parsed = [str(i).strip() for i in items if str(i).strip()]
            return parsed[:3]
    except Exception:
        pass

    # å…œåº•ï¼šæŒ‰è¡Œè§£æ
    result: list[str] = []
    for line in text.splitlines():
        cleaned = line.strip()
        cleaned = cleaned.lstrip("-â€¢")
        cleaned = cleaned.strip()
        cleaned = cleaned.removeprefix("1.").removeprefix("2.").removeprefix("3.").strip()
        if cleaned:
            result.append(cleaned)
        if len(result) >= 3:
            break
    return result[:3]


def _build_follow_up_prompt(user_msg: str, assistant_reply: str) -> str:
    return f"""ä½ æ˜¯ä¸€ä¸ªå¯¹è¯åŠ©æ‰‹ï¼Œè¯·åŸºäºæœ¬è½®é—®ç­”ç”Ÿæˆ 3 æ¡åç»­å»ºè®®ã€‚
è¦æ±‚ï¼š
1) å¿…é¡»å’Œç”¨æˆ·å½“å‰ä¸»é¢˜å¼ºç›¸å…³ï¼Œå¸®åŠ©ç”¨æˆ·ç»§ç»­æ¢ç´¢ã€‚
2) æ ¹æ®è¯­å¢ƒè‡ªåŠ¨é€‰æ‹©è¾“å‡ºå½¢æ€ï¼š
   - å¦‚æœç”¨æˆ·æ˜æ˜¾è¿˜åœ¨æ¢ç´¢/æ¯”è¾ƒ/å†³ç­–ï¼Œè¾“å‡ºâ€œè¿½é—®å¥â€ï¼ˆå»ºè®®ä»¥é—®å·ç»“å°¾ï¼‰ã€‚
   - å¦‚æœæœ¬è½®å›ç­”å·²è¾ƒå®Œæ•´ï¼Œè¾“å‡ºâ€œå…³è”è¯é¢˜çŸ­è¯­â€ï¼ˆä¸åŠ å¥å·ã€ä¸åŠ è§£é‡Šï¼Œä¸å†™å®Œæ•´é™ˆè¿°å¥ï¼‰ã€‚
3) ä¸‰æ¡ä¿æŒåŒä¸€é£æ ¼ï¼ˆå…¨æ˜¯è¿½é—®å¥æˆ–å…¨æ˜¯è¯é¢˜çŸ­è¯­ï¼‰ï¼Œé¿å…é‡å¤ã€‚
4) è¿½é—®å¥å»ºè®® 8-24 ä¸ªä¸­æ–‡å­—ç¬¦ï¼›è¯é¢˜çŸ­è¯­å»ºè®® 4-12 ä¸ªä¸­æ–‡å­—ç¬¦ã€‚
5) ä¸è¦å‡ºç°è§£é‡Šæ–‡å­—ã€‚
6) ä»…è¿”å› JSON æ•°ç»„å­—ç¬¦ä¸²ï¼Œä¾‹å¦‚ï¼š[\"å»ºè®®1\", \"å»ºè®®2\", \"å»ºè®®3\"]

ç”¨æˆ·é—®é¢˜ï¼š{user_msg}
åŠ©æ‰‹å›ç­”ï¼š{assistant_reply}
"""


async def _generate_follow_ups(llm: ChatOpenAI, user_msg: str, assistant_reply: str) -> list[str]:
    """ç”Ÿæˆåç»­è¿½é—®å»ºè®®ã€‚"""
    if not assistant_reply.strip():
        return []

    prompt = _build_follow_up_prompt(user_msg, assistant_reply)
    try:
        resp = await llm.ainvoke([HumanMessage(content=prompt)])
        return _parse_follow_ups(resp.content or "")
    except Exception:
        return []



async def chat(user_msg: str, conversation_id: str | None = None) -> AsyncGenerator[str, None]:
    """å¤„ç†ç”¨æˆ·æ¶ˆæ¯ï¼Œæµå¼è¿”å›å“åº”ã€‚

    äº§å‡º Server-Sent Events (SSE) æ ¼å¼çš„ JSON chunksã€‚
    """
    # 1. åˆ›å»ºæˆ–è·å–ä¼šè¯
    if not conversation_id or not db.get_conversation(conversation_id):
        conversation_id = db.create_conversation(_generate_title(user_msg))

    # 2. ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
    db.save_message(conversation_id, "user", user_msg)

    # 3. åŠ è½½å†å²
    history = db.get_messages(conversation_id, limit=20)

    # 4. æ„å»ºæ¶ˆæ¯
    messages = _build_messages(history[:-1], user_msg)  # æ’é™¤åˆšå­˜çš„ç”¨æˆ·æ¶ˆæ¯

    # 5. åˆå§‹åŒ– LLMï¼ˆç»‘å®šå·¥å…·ï¼‰
    llm = _get_llm()
    llm_with_tools = llm.bind_tools(ALL_TOOLS)

    # 6. è°ƒç”¨ LLM
    all_cards = []
    full_content = ""
    message_id = uuid.uuid4().hex[:16]

    try:
        # ç¬¬ä¸€è½®ï¼šLLM å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·
        import logging
        logger = logging.getLogger("agent")
        logger.info("[Agent] ç¬¬ä¸€è½®è°ƒç”¨ LLM (ainvoke)...")
        response = await llm_with_tools.ainvoke(messages)
        logger.info(f"[Agent] ç¬¬ä¸€è½®å®Œæˆ â€” contenté•¿åº¦={len(response.content or '')}, tool_callsæ•°é‡={len(response.tool_calls or [])}")

        # å¤„ç†å·¥å…·è°ƒç”¨
        if response.tool_calls:
            # å¦‚æœ LLM åŒæ—¶è¿”å›äº†æ–‡æœ¬å†…å®¹ï¼ˆå¦‚"å¥½çš„ï¼Œæ­£åœ¨ä¸ºæ‚¨æŸ¥è¯¢â€¦"ï¼‰ï¼Œå…ˆå‘ç»™å‰ç«¯
            if response.content:
                full_content += response.content
                token_chunk = json.dumps({
                    "type": "token",
                    "content": response.content,
                }, ensure_ascii=False)
                yield f"data: {token_chunk}\n\n"

            # æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
            tool_messages = [response]
            for tool_call in response.tool_calls:
                tool_name = tool_call["name"]
                tool_args = tool_call["args"]
                tool_fn = TOOL_MAP.get(tool_name)
                logger.info(f"[Agent] è°ƒç”¨å·¥å…·: {tool_name}, å‚æ•°: {tool_args}")

                if tool_fn:
                    try:
                        result = await tool_fn.ainvoke(tool_args)
                        if isinstance(result, str):
                            result = json.loads(result)
                        logger.info(f"[Agent] å·¥å…· {tool_name} è¿”å›æˆåŠŸ")
                    except Exception as e:
                        logger.error(f"[Agent] å·¥å…· {tool_name} æ‰§è¡Œå¤±è´¥: {e}")
                        result = {"error": str(e)}

                    # æå–å¡ç‰‡
                    cards = _extract_cards(tool_name, result)
                    all_cards.extend(cards)

                    # å‘é€å¡ç‰‡ chunk
                    for card in cards:
                        card_chunk = json.dumps({
                            "type": "card",
                            "card": card,
                        }, ensure_ascii=False)
                        yield f"data: {card_chunk}\n\n"

                    # æ„å»ºå·¥å…·å“åº”æ¶ˆæ¯
                    from langchain_core.messages import ToolMessage
                    tool_messages.append(
                        ToolMessage(
                            content=json.dumps(result, ensure_ascii=False),
                            tool_call_id=tool_call["id"],
                        )
                    )
                else:
                    logger.warning(f"[Agent] æœªæ‰¾åˆ°å·¥å…·: {tool_name}")

            # ç¬¬äºŒè½®ï¼šLLM åŸºäºå·¥å…·ç»“æœç”Ÿæˆæœ€ç»ˆå›ç­”
            logger.info("[Agent] ç¬¬äºŒè½®è°ƒç”¨ LLM (astream)...")
            messages.extend(tool_messages)
            async for chunk in llm.astream(messages):
                if chunk.content:
                    full_content += chunk.content
                    token_chunk = json.dumps({
                        "type": "token",
                        "content": chunk.content,
                    }, ensure_ascii=False)
                    yield f"data: {token_chunk}\n\n"
            logger.info(f"[Agent] ç¬¬äºŒè½®å®Œæˆ â€” æ€»å›å¤é•¿åº¦={len(full_content)}")

        else:
            # æ— å·¥å…·è°ƒç”¨ï¼Œç›´æ¥æµå¼è¾“å‡º
            logger.info("[Agent] æ— å·¥å…·è°ƒç”¨ï¼Œç›´æ¥æµå¼è¾“å‡º...")
            async for chunk in llm_with_tools.astream(messages):
                if chunk.content:
                    full_content += chunk.content
                    token_chunk = json.dumps({
                        "type": "token",
                        "content": chunk.content,
                    }, ensure_ascii=False)
                    yield f"data: {token_chunk}\n\n"

    except Exception as e:
        error_msg = f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶é‡åˆ°äº†é—®é¢˜ï¼š{str(e)}"
        full_content = error_msg
        error_chunk = json.dumps({
            "type": "error",
            "content": error_msg,
        }, ensure_ascii=False)
        yield f"data: {error_chunk}\n\n"

    # 7. ç”Ÿæˆå¹¶å‘é€ follow-up é—®é¢˜
    follow_ups = await _generate_follow_ups(llm, user_msg, full_content)
    if follow_ups:
        follow_up_chunk = json.dumps({
            "type": "follow_ups",
            "questions": follow_ups,
        }, ensure_ascii=False)
        yield f"data: {follow_up_chunk}\n\n"

    # 8. ä¿å­˜åŠ©æ‰‹æ¶ˆæ¯
    db.save_message(
        conversation_id,
        "assistant",
        full_content,
        cards=[c for c in all_cards] if all_cards else None,
        follow_ups=follow_ups if follow_ups else None,
    )

    # 9. æ›´æ–°ä¼šè¯æ ‡é¢˜ï¼ˆé¦–æ¬¡å¯¹è¯ï¼‰
    if len(history) <= 1:
        db.update_conversation_title(conversation_id, _generate_title(user_msg))

    # 10. å‘é€å®Œæˆ chunk
    done_chunk = json.dumps({
        "type": "done",
        "conversation_id": conversation_id,
        "message_id": message_id,
    }, ensure_ascii=False)
    yield f"data: {done_chunk}\n\n"
